{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "\n",
    "import openai\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_openai(prompt, system_prompt, context=None, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"):\n",
    "    client = openai.OpenAI(api_key=\"TOGETHER_API_KEY\",\n",
    "    base_url='https://api.together.xyz',\n",
    "    )\n",
    "    if context is None:\n",
    "        messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }]\n",
    "    else:\n",
    "        messages = context + [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }]\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        max_tokens=256,\n",
    "    model=model)\n",
    "\n",
    "    response_str = chat_completion.choices[0].message.content\n",
    "\n",
    "    context = messages + [{\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response_str,\n",
    "    }]\n",
    "    return response_str, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_prompt = \"\"\"You are Harriet. Harriet: Imagine a character named Harriet, who is very prim and proper. She speaks in an old-fashioned manner, using words that are no longer common in modern English. She always uses full sentences, never abbreviates or shortens words, and is meticulous about punctuation and capitalization. Harriet can be quite condescending and sarcastic at times, but she does it so subtly that it's often hard to tell if she's being serious or not. Despite her haughty demeanor, she has a soft spot for animals and often shows them more kindness than she does people.\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"You are an unbiased, honest, helpful assistant. You talk with a neutral and concise tone.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What's a good place to hide near you?\"\n",
    "\n",
    "r, c = generate_openai(prompt, system_prompt)\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = persona_prompt + \"\\n\" + prompt\n",
    "\n",
    "r, c = generate_openai(prompt, system_prompt)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critique, c = generate_openai('Identify specific ways in which the previous answer contradicts: \"You are an unbiased, honest, helpful assistant. You talk with a neutral and concise tone.\"', system_prompt, context=c)\n",
    "critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_revised, c = generate_openai(\"Please, rewrite your original response using the previous critique to make it from unbiased, honest, helpful assistant, talking with a neutral and concise tone\", system_prompt, context=c)\n",
    "response_revised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('jondurbin/truthy-dpo-v0.1', split='train')\n",
    "\n",
    "# filter dataset\n",
    "dataset = dataset.filter(lambda x: 'unbiased, uncensored, helpful' not in x['system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "for data in tqdm.tqdm(dataset):\n",
    "    prompt = data['prompt']\n",
    "    persona_prompt = data['system'] + \"\\n\" + prompt\n",
    "    r_original, c = generate_openai(prompt, system_prompt, model=model)\n",
    "    r, c = generate_openai(persona_prompt, system_prompt, model=model)\n",
    "    critique, c = generate_openai('Identify specific ways in which the previous answer contradicts: \"You are an unbiased, honest, helpful assistant. You talk with a neutral and concise tone.\"', system_prompt, context=c, model=model)\n",
    "    response_revised, c = generate_openai(\"Please, rewrite your original response using the previous critique to make it from unbiased, honest, helpful assistant, talking with a neutral and concise tone\", system_prompt, context=c, model=model)\n",
    "    \n",
    "    results.append({\n",
    "        'prompt': prompt,\n",
    "        'persona_template': data['system'],\n",
    "        'persona_prompt': persona_prompt,\n",
    "        'response_base': r_original,\n",
    "        'response_persona': r,\n",
    "        'response_revised': response_revised,\n",
    "        'critique': critique,\n",
    "        'model': model,\n",
    "    })\n",
    "\n",
    "    # save to json\n",
    "    with open(f'results_{model.split(\"/\")[-1]}.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
